name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop, 'copilot/**' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  # Quality gates and linting
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff black isort
    
    - name: Run Black
      run: black --check src config tests
    
    - name: Run Ruff
      run: ruff check src config tests
    
    - name: Run isort
      run: isort --check-only src config tests

  # Unit tests with multiple Python versions
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libopencv-dev python3-opencv
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing]'
    
    - name: Run unit tests
      run: |
        pytest tests/ -m unit -v --tb=short --cov=src --cov=config --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: unit-${{ matrix.python-version }}

  # Integration tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libopencv-dev python3-opencv
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing]'
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --tb=short --cov=src --cov=config --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: integration-tests

  # End-to-end tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libopencv-dev python3-opencv
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing]'
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -v --tb=short --cov=src --cov=config --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: e2e-tests

  # Performance tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libopencv-dev python3-opencv
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing,monitoring,performance]'
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ -v --tb=short --benchmark-only --benchmark-json=benchmark-results.json
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'push'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: false
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: benchmark-results.json
        retention-days: 30

  # Load testing
  load-tests:
    name: Load Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing,performance]'
    
    - name: Start application
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        python app.py &
        sleep 10
    
    - name: Run load tests
      run: |
        locust -f tests/load/locustfile.py \
          --host=http://localhost:5000 \
          --headless \
          --users 50 \
          --spawn-rate 5 \
          --run-time 60s \
          --html load-test-report.html \
          --csv load-test-results
    
    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-reports
        path: |
          load-test-report.html
          load-test-results*.csv
        retention-days: 30

  # Security tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing,security]'
    
    - name: Run security tests
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        pytest tests/security/ -v -m security --tb=short --junit-xml=security-test-results.xml
    
    - name: Run Bandit security scan
      run: |
        bandit -r src/ config/ -f json -o bandit-report.json || true
        bandit -r src/ config/ -f txt
    
    - name: Run Safety dependency check
      run: |
        safety check --json --output safety-report.json || true
        safety check --short || true
    
    - name: Generate security summary
      if: always()
      run: |
        echo "## ðŸ”’ Security Scan Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Bandit summary
        if [ -f bandit-report.json ]; then
          echo "### Bandit Security Scan" >> $GITHUB_STEP_SUMMARY
          python -c "
          import json
          with open('bandit-report.json') as f:
              data = json.load(f)
              metrics = data.get('metrics', {})
              total = sum(v.get('SEVERITY.HIGH', 0) + v.get('SEVERITY.MEDIUM', 0) for v in metrics.values())
              print(f'Found {total} potential issues')
          " >> $GITHUB_STEP_SUMMARY || echo "âœ… No issues found" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
    
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          security-test-results.xml
        retention-days: 30

  # Coverage report
  coverage-report:
    name: Generate Coverage Report
    needs: [unit-tests, integration-tests, e2e-tests]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libopencv-dev python3-opencv
    
    - name: Install package
      run: |
        python -m pip install --upgrade pip
        pip install -e '.[dev,testing]'
    - name: Run all tests with coverage
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        pytest tests/ -v \
          --cov=src --cov=config \
          --cov-report=html \
          --cov-report=xml \
          --cov-report=json \
          --junit-xml=pytest-results.xml \
          --html=test-report.html --self-contained-html
    
    - name: Check coverage threshold
      run: |
        python scripts/testing/generate_coverage_report.py --fail-under 80 --markdown
    
    - name: Generate coverage summary for PR
      if: github.event_name == 'pull_request'
      run: |
        python scripts/testing/generate_coverage_report.py --no-run --markdown
        
        # Add coverage comment to PR
        if [ -f COVERAGE_REPORT.md ]; then
          cat COVERAGE_REPORT.md >> $GITHUB_STEP_SUMMARY
        fi
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-reports
        path: |
          htmlcov/
          coverage.xml
          coverage.json
          pytest-results.xml
          test-report.html
          COVERAGE_REPORT.md
        retention-days: 30
    
    - name: Upload to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: all-tests
        fail_ci_if_error: false

  # Quality gates
  quality-gates:
    name: Quality Gates
    needs: [code-quality, unit-tests, integration-tests, e2e-tests, performance-tests, security-tests, coverage-report]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Check all tests passed
      run: |
        echo "## ðŸš¦ Quality Gates Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| E2E Tests | ${{ needs.e2e-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Tests | ${{ needs.performance-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Tests | ${{ needs.security-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Coverage Report | ${{ needs.coverage-report.result }} |" >> $GITHUB_STEP_SUMMARY
        
        # Check if any critical jobs failed
        if [[ "${{ needs.code-quality.result }}" == "failure" || 
              "${{ needs.unit-tests.result }}" == "failure" || 
              "${{ needs.integration-tests.result }}" == "failure" ]]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âŒ **Critical tests failed!**" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… **All quality gates passed!**" >> $GITHUB_STEP_SUMMARY
