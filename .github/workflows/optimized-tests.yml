name: "Optimized Test Suite"

# This workflow implements comprehensive test infrastructure hardening
# with performance optimizations as described in Issue #27

on:
  push:
    branches: [ master, main, develop, 'copilot/**' ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
      - '.github/ISSUE_TEMPLATE/**'
  pull_request:
    branches: [ master, main, develop ]
    paths-ignore:
      - '*.md'
      - 'docs/**'
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      full_suite:
        description: 'Run full test suite (including slow tests)'
        required: false
        default: 'false'

permissions:
  contents: read
  pull-requests: write
  checks: write

env:
  PYTHON_VERSION_MATRIX: '["3.10", "3.11", "3.12"]'
  MAIN_PYTHON_VERSION: '3.11'
  # Performance tuning
  PYTEST_XDIST_AUTO_NUM_WORKERS: 'auto'
  PIP_DISABLE_PIP_VERSION_CHECK: '1'
  PIP_NO_CACHE_DIR: '0'  # Enable cache

jobs:
  # Job 1: Code Quality (Fast feedback)
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Needed for smart test selection
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.MAIN_PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements-ci.txt
          requirements-testing.txt
    
    - name: Cache pre-commit environments
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}
        restore-keys: |
          pre-commit-${{ runner.os }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip wheel setuptools
        pip install ruff black isort
    
    - name: Run Ruff (fast linter)
      run: ruff check src config tests --output-format=github
      continue-on-error: true
    
    - name: Run Black
      run: black --check src config tests
      continue-on-error: true
    
    - name: Run isort
      run: isort --check-only src config tests
      continue-on-error: true

  # Job 2: Smart Test Selection and Unit Tests
  unit-tests:
    name: "Unit Tests (Python ${{ matrix.python-version }})"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Needed for smart test selection
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: |
          requirements-ci.txt
          requirements-testing.txt
    
    - name: Cache system dependencies (Ubuntu)
      uses: actions/cache@v4
      with:
        path: |
          /var/cache/apt
          ~/.cache/pip
        key: ${{ runner.os }}-system-deps-${{ hashFiles('.github/workflows/optimized-tests.yml') }}
        restore-keys: |
          ${{ runner.os }}-system-deps-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update -qq
        sudo apt-get install -y --no-install-recommends \
          build-essential \
          cmake \
          libopencv-dev \
          python3-opencv \
          libgl1-mesa-glx \
          libglib2.0-0 \
          libsm6 \
          libxext6 \
          libxrender-dev \
          libgomp1
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip wheel setuptools
        pip install -r requirements-ci.txt
        pip install pytest-timeout pytest-xdist
    
    - name: Smart test selection
      id: test-selection
      run: |
        python scripts/ci/smart_test_selection.py \
          --base-branch ${{ github.base_ref || 'master' }} \
          --output-file selected-tests.txt \
          --format paths || echo "Smart selection unavailable, running all tests"
        
        if [ -f selected-tests.txt ] && [ -s selected-tests.txt ]; then
          echo "smart_selection=true" >> $GITHUB_OUTPUT
          echo "Selected tests:"
          cat selected-tests.txt
        else
          echo "smart_selection=false" >> $GITHUB_OUTPUT
          echo "Running full test suite"
        fi
      continue-on-error: true
    
    - name: Run unit tests with parallel execution
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        # Calculate optimal worker count (max 4 to avoid resource contention)
        CPU_COUNT=$(python -c "import os; print(min(os.cpu_count() or 1, 4))")
        echo "Running tests with $CPU_COUNT workers"
        
        # Run tests with smart selection if available
        if [ "${{ steps.test-selection.outputs.smart_selection }}" = "true" ] && [ -f selected-tests.txt ]; then
          TEST_FILES=$(cat selected-tests.txt | tr '\n' ' ')
          python -m pytest $TEST_FILES -v \
            -n $CPU_COUNT \
            --dist worksteal \
            --cov=src --cov=config \
            --cov-report=xml --cov-report=term-missing \
            --cov-report=json \
            --junit-xml=pytest-results.xml \
            --timeout=300 \
            --maxfail=10 \
            -m "not (hardware or gpu or load or e2e or slow)" || {
            echo "⚠️ Some tests failed"
            exit 1
          }
        else
          # Run full test suite
          python -m pytest tests/ -v \
            -n $CPU_COUNT \
            --dist worksteal \
            --cov=src --cov=config \
            --cov-report=xml --cov-report=term-missing \
            --cov-report=json \
            --junit-xml=pytest-results.xml \
            --timeout=300 \
            --maxfail=10 \
            -m "not (hardware or gpu or load or e2e or slow)" || {
            echo "⚠️ Some tests failed"
            exit 1
          }
        fi
    
    - name: Check performance regressions
      if: always()
      run: |
        python scripts/ci/performance_monitor.py \
          --junit-xml pytest-results.xml \
          --baseline tests/baselines/performance.json \
          --threshold 0.2 \
          --report-file performance-regressions.json || {
          echo "⚠️ Performance monitoring skipped or found regressions"
          exit 0
        }
      continue-on-error: true
    
    - name: Generate test summary
      if: always()
      run: |
        python scripts/ci/github_test_summary.py \
          --junit-xml pytest-results.xml \
          --coverage-json coverage.json \
          --performance-json performance-regressions.json \
          >> $GITHUB_STEP_SUMMARY || {
          echo "⚠️ Test summary generation failed"
        }
      continue-on-error: true
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-unit-py${{ matrix.python-version }}
        path: |
          pytest-results.xml
          coverage.xml
          coverage.json
          performance-regressions.json
        retention-days: 30
    
    - name: Upload coverage to Codecov
      if: matrix.python-version == env.MAIN_PYTHON_VERSION
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: unit-${{ matrix.python-version }}
        fail_ci_if_error: false

  # Job 3: Integration Tests (with caching)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [code-quality]
    timeout-minutes: 20
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.MAIN_PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Cache system dependencies
      uses: actions/cache@v4
      with:
        path: |
          /var/cache/apt
          ~/.cache/pip
        key: ${{ runner.os }}-integration-${{ hashFiles('.github/workflows/optimized-tests.yml') }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update -qq
        sudo apt-get install -y --no-install-recommends \
          build-essential cmake libopencv-dev python3-opencv libgl1-mesa-glx
        python -m pip install --upgrade pip wheel setuptools
        pip install -r requirements-ci.txt
        pip install pytest-timeout pytest-xdist
    
    - name: Run integration tests with parallelism
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        CPU_COUNT=$(python -c "import os; print(min(os.cpu_count() or 1, 2))")
        python -m pytest tests/integration/ -v \
          -n $CPU_COUNT \
          --dist worksteal \
          --cov=src --cov=config \
          --cov-report=xml \
          --junit-xml=pytest-results-integration.xml \
          --timeout=300 \
          --maxfail=5 || {
          echo "⚠️ Some integration tests failed"
          exit 0
        }
      continue-on-error: true
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-integration
        path: |
          pytest-results-integration.xml
          coverage.xml
        retention-days: 30

  # Job 4: Performance Tests (only on main branch)
  performance-tests:
    name: Performance & Benchmarks
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main')
    timeout-minutes: 15
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.MAIN_PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip wheel setuptools
        pip install -r requirements-ci.txt
        pip install pytest-benchmark memory-profiler psutil
    
    - name: Run performance tests
      env:
        DEVELOPMENT_MODE: "true"
        PYTHONPATH: ${{ github.workspace }}
      run: |
        python -m pytest tests/performance/ -v \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --junit-xml=pytest-results-performance.xml || {
          echo "⚠️ Performance tests not available or failed"
          echo '{"benchmarks":[]}' > benchmark-results.json
        }
      continue-on-error: true
    
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: |
          benchmark-results.json
          pytest-results-performance.xml
        retention-days: 30

  # Job 5: Quality Gates
  quality-gates:
    name: Quality Gates
    needs: [code-quality, unit-tests, integration-tests]
    runs-on: ubuntu-latest
    if: always()
    timeout-minutes: 5
    
    steps:
    - name: Check quality metrics
      run: |
        echo "## 🚦 Quality Gates Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        
        # Only fail on critical job failures
        if [[ "${{ needs.unit-tests.result }}" == "failure" ]]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "❌ **Unit tests failed - please fix**" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi
        
        if [[ "${{ needs.code-quality.result }}" == "failure" ]]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "⚠️ **Code quality issues detected - please review**" >> $GITHUB_STEP_SUMMARY
          # Don't fail CI for linting in initial implementation
          exit 0
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "✅ **Quality gates passed!**" >> $GITHUB_STEP_SUMMARY
